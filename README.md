<div align="center">

  <img src="https://readme-typing-svg.herokuapp.com?font=Fira+Code&size=28&duration=4200&pause=1200&color=00C4B4&center=true&vCenter=true&width=520&lines=AI+Systems+%26+Large+Scale+Inference;Trustworthy+ML+%7C+Efficient+Training+%7C+Systems+for+AGI;Exploring+the+Next+Paradigm+of+Intelligence" alt="Typing SVG" />

  <br/><br/>

  <img src="https://img.shields.io/badge/Research_Focus-Frontier_AI-orange?style=for-the-badge&logoColor=white&logo=arxiv" height="28"/>
  <img src="https://img.shields.io/badge/Current_Interest-MoE_%7C_Sparse_Expert_%7C_Test%2DTime_Scaling-9C27B0?style=for-the-badge&logoColor=white" height="28"/>
  <img src="https://img.shields.io/badge/Scale-10B‚Üí405B‚ÜíTrillion_Parameter_era-FF5722?style=for-the-badge&logoColor=white" height="28"/>

</div>

<br/>

### üåå Currently Exploring (2025‚Äì2026)

- **Post-training compute-optimal scaling laws** for reasoning & long-context models  
- **Mixture-of-Experts routing at inference-time scale** (dynamic routing + speculative decoding synergy)  
- **Test-time compute scaling** with process supervision & self-verification  
- **Memory-efficient training** for models > 500B parameters on heterogeneous clusters  
- **Mechanistic interpretability** of frontier sparse & multimodal models  

<br/>

### üî¨ Selected Research Artifacts & Open-source Contributions

| Project | Description | Status | Links |
|-------|---------------------------------------------|--------|-------|
| **Apex-MoE** | Next-generation MoE training & inference framework (vLLM + DeepSpeed compatible) | Active | [GitHub](https://github.com/apex-moe) |
| **SpecInfer-Tree** | Speculative inference with adaptive draft tree generation | Paper + Code | [arXiv 250x.xxxxx](https://arxiv.org/abs/25xx.xxxxx) |
| **LongGuard** | Context poisoning & jailbreak defense for 1M+ token windows | In use @ several labs | [repo](https://github.com/long-guard) |
| **ŒºTorch** | Ultra-lightweight autograd + distributed training kernel library | Educational / Research | [ŒºTorch](https://github.com/micro-torch) |
| **OpenR1** | Fully open reproduction of frontier reasoning models (post-training only) | Community-driven | Tracking progress |

<br/>

### üß¨ Currently Reading / Thinking About

- **Scaling Monosemanticity** (Anthropic 2024‚Äì2025 updates)  
- **Inference-time scaling of verifier-based tree search** (Œ±Œ≤-style + MCTS variants)  
- **Conditional compute & dynamic depth/width allocation** in Transformers  
- **Mixture-of-Transformers & heterogeneous expert specialization**  
- **Whether gradient-based routing can ever beat hash/random routing at trillion scale**

<br/>

<div align="center">

  <img src="https://img.shields.io/badge/Let's_talk-DeepSeek-R1%20scale%20laws-00BCD4?style=for-the-badge&logoColor=white" height="32"/>
  &nbsp;&nbsp;
  <img src="https://img.shields.io/badge/DM_me-@your_twitter-1DA1F2?style=for-the-badge&logo=twitter&logoColor=white" height="32"/>

  <br/><br/>

  <sub>‚ÄúThe scarce resource in the AGI era is no longer parameters or FLOPs ‚Äî it is <strong>post-training quality compute</strong>.‚Äù</sub>

</div>
